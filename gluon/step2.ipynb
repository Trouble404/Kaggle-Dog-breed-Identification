{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Loading data to gluon and obtain features of images\n",
    "** Extract features based on [Gluon Model Zoo](https://mxnet.incubator.apache.org/versions/master/api/python/gluon/model_zoo.html)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "from mxnet import image\n",
    "from mxnet import init\n",
    "from mxnet import nd\n",
    "from mxnet.gluon.data import vision\n",
    "from mxnet.gluon.model_zoo import vision as models\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Image API in MXNet](https://mxnet.incubator.apache.org/api/python/image/image.html)**\n",
    "\n",
    "**[Gluon Data API](https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=imagefolderdataset#mxnet.gluon.data.vision.ImageFolderDataset)**\n",
    "\n",
    "**[Fine-tuning: 通过微调来迁移学习](http://zh.gluon.ai/chapter_computer-vision/fine-tuning.html)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Define the pre-processing function\n",
    "\n",
    "**这里需要mean和std同时都设置正确才能进行预处理，如果你只设置了mean，没有设置std，那么还是没有启动归一化的预处理。\n",
    " 这里主要调用ColorNormalizeAug()函数，这个函数调用color_normalize()函数，这个函数的实现很简单，\n",
    " 就是将原图像的像素值减去均值mean，然后除以标准差std得到返回值。**\n",
    "![image](https://raw.githubusercontent.com/Trouble404/Kaggle-Dog-breed-Identification/master/readme_pic_add/colornormalizeaug.PNG)\n",
    "\n",
    "**resize这个参数很重要，一般都要做resize，如果你的resize参数设置为224，你的原图像是350X300，那么最后resize的大小就是\n",
    " (350X300/224)X224。这里ResizeAug()函数调用resize_short()函数，resize_short()函数调用OpenCV的imresize()函数完成resize\n",
    " ，interp参数为2表示采用双三次插值做resize。**\n",
    "![iamge](https://raw.githubusercontent.com/Trouble404/Kaggle-Dog-breed-Identification/master/readme_pic_add/forceresizeaug.PNG)\n",
    "\n",
    "**Pretrained models are converted from torchvision. All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (N x 3 x H x W), where N is the batch size, and H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using ```mean = [0.485, 0.456, 0.406]``` and ```std = [0.229, 0.224, 0.225]```. The transformation should preferrably happen at preprocessing. **\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ctx = mx.gpu() # use cpu   gpu will cause python stop working here\n",
    "\n",
    "preprocessing = [\n",
    "    image.ForceResizeAug((224,224)), # Make resize shorter edge to size augmenter.\n",
    "    image.ColorNormalizeAug(mean=nd.array([0.485, 0.456, 0.406]), std=nd.array([0.229, 0.224, 0.225])) # Mean and std normalization\n",
    "]\n",
    "\n",
    "def transform(data, label):\n",
    "    data = data.astype('float32') / 255  # transform type to float and do normalization\n",
    "    for pre in preprocessing:\n",
    "        data = pre(data) # preprocessing data\n",
    "    \n",
    "    data = nd.transpose(data, (2,0,1)) # Transpose image to N, H, W  Kaggle image is BGR but we need RGB here\n",
    "    return data, nd.array([label]).asscalar().astype('float32') # return to ndARRARY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 define the output feature\n",
    "![image](https://raw.githubusercontent.com/Trouble404/Kaggle-Dog-breed-Identification/master/readme_pic_add/asincontent.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_features(net, data):\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    for X, y in tqdm(data): # tqdm to display the schedule\n",
    "        feature = net.features(X.as_in_context(ctx)) # return target arrary of image features\n",
    "        features.append(feature.asnumpy()) # asnumpy() -> convert to numpy array  superposition features\n",
    "        labels.append(y.asnumpy()) # superposition labels\n",
    "    \n",
    "    features = np.concatenate(features, axis=0) # splice big data fast\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 obtain feature vector\n",
    "\n",
    "**inception_v3 model need resize at least 299, other models need resize at least 244**\n",
    "![iamge](https://raw.githubusercontent.com/Trouble404/Kaggle-Dog-breed-Identification/master/readme_pic_add/loading.png)\n",
    "\n",
    "![iamge](https://raw.githubusercontent.com/Trouble404/Kaggle-Dog-breed-Identification/master/readme_pic_add/dataloder.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preprocessing[0] = image.ForceResizeAug((224,224))\n",
    "imgs = vision.ImageFolderDataset('for_train', transform=transform) # read sorted images\n",
    "data = gluon.data.DataLoader(imgs, 4) # load images with batch_zise 4 due to graphical is GTX960M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VGG-16 model with batch normalization from the [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556) paper.**\n",
    "\n",
    "**ResNet-152 V1 model from [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) paper.**\n",
    "\n",
    "**Densenet-BC 161-layer model from the [Densely Connected Convolutional Networks](https://arxiv.org/pdf/1608.06993.pdf) paper.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2556/2556 [10:10<00:00,  4.19it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2556/2556 [12:02<00:00,  3.54it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2556/2556 [11:17<00:00,  3.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 33min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features_vgg, labels = get_features(models.vgg16_bn(pretrained=True, ctx=ctx), data)\n",
    "features_resnet, _ = get_features(models.resnet152_v1(pretrained=True, ctx=ctx), data)\n",
    "features_densenet, _ = get_features(models.densenet161(pretrained=True, ctx=ctx), data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inception v3 model from [Rethinking the Inception Architecture for Computer Vision](https://arxiv.org/abs/1512.00567) paper.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessing[0] = image.ForceResizeAug((299,299))\n",
    "imgs_299 = vision.ImageFolderDataset('for_train', transform=transform)\n",
    "data_299 = gluon.data.DataLoader(imgs_299, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2556/2556 [07:42<00:00,  5.52it/s]\n",
      "E:\\Anaconda\\envs\\MXNet\\lib\\site-packages\\jupyter_client\\jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2018-02-13 15:18:47.631777. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    }
   ],
   "source": [
    "features_inception, _ = get_features(models.inception_v3(pretrained=True, ctx=ctx), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py  # compress and save features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with h5py.File('features_train.h5', 'w') as f:\n",
    "    f['vgg'] = features_vgg\n",
    "    f['resnet'] = features_resnet\n",
    "    f['densenet'] = features_densenet\n",
    "    f['inception'] = features_inception\n",
    "    f['labels'] = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.86it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  2.01it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.48it/s]\n",
      "E:\\Anaconda\\envs\\MXNet\\lib\\site-packages\\jupyter_client\\jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2018-02-13 15:18:49.681827. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    }
   ],
   "source": [
    "preprocessing[0] = image.ForceResizeAug((224,224))\n",
    "imgs = vision.ImageFolderDataset('for_test', transform=transform)\n",
    "data = gluon.data.DataLoader(imgs, 4)\n",
    "\n",
    "features_vgg, _ = get_features(models.vgg16_bn(pretrained=True, ctx=ctx), data)\n",
    "features_resnet, _ = get_features(models.resnet152_v1(pretrained=True, ctx=ctx), data)\n",
    "features_densenet, _ = get_features(models.densenet161(pretrained=True, ctx=ctx), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\envs\\MXNet\\lib\\site-packages\\jupyter_client\\jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2018-02-13 15:18:50.347764. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    }
   ],
   "source": [
    "preprocessing[0] = image.ForceResizeAug((299,299))\n",
    "imgs_299 = vision.ImageFolderDataset('for_test', transform=transform)\n",
    "data_299 = gluon.data.DataLoader(imgs_299, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  2.02it/s]\n",
      "E:\\Anaconda\\envs\\MXNet\\lib\\site-packages\\jupyter_client\\jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2018-02-13 15:18:51.051728. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    }
   ],
   "source": [
    "features_inception, _ = get_features(models.inception_v3(pretrained=True, ctx=ctx), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File('features_test.h5', 'w') as f:\n",
    "    f['vgg'] = features_vgg\n",
    "    f['resnet'] = features_resnet\n",
    "    f['densenet'] = features_densenet\n",
    "    f['inception'] = features_inception"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:MXNet]",
   "language": "python",
   "name": "conda-env-MXNet-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
